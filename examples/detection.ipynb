{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSMOS Detection pipeline\n",
    "\n",
    "This walkthrough shows how to perform basic document region detection over PDF documents using COSMOS. We'll use a trained model to classify regions on pages into tables, equations, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from ingest.utils.pdf_helpers import prepare_pdf_objs\n",
    "from ingest.ingest import pdf_to_images\n",
    "from ingest.process_page import process_page\n",
    "from ingest.process.detection.src.infer import get_model\n",
    "from ingest.detect import detect\n",
    "from ingest.detect_setup import DetectPlugin\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the distributed backend\n",
    "\n",
    "Cosmos uses Dask's lower level api, [Dask Distributed](https://distributed.dask.org/en/latest/) to handle its processing load. In this walkthrough, we'll setup a single node for processing, but this can be naturally extended to however many workers you want. Now, you can visualize your processing by clicking on the dashboard link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:asyncio:Using selector: KqueueSelector\n",
      "DEBUG:asyncio:Using selector: KqueueSelector\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:58661</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>17.18 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:58661' processes=4 threads=8, memory=17.18 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(serializers=['msgpack', 'dask'], deserializers=['msgpack', 'dask']) # We'll need msgpack as a serializer to fallback on\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the PDF files\n",
    "\n",
    "Next, we can prepare the PDF files. We'll use the documents in the example_docs directory, but you can point to your own documents to try it out. We also create a dataset id, which we will use to access our results later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_directory = './example_docs'\n",
    "dataset_id = 'example'\n",
    "pdfs = client.submit(prepare_pdf_objs, documents_directory, dataset_id)\n",
    "pdfs = pdfs.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the images from the pdfs\n",
    "\n",
    "We're going to not use subtasks. As a result we're going to need to call result() to synchronize and flatten lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axg160/miniconda3/envs/cosmos2/lib/python3.6/site-packages/distributed/worker.py:3378: UserWarning: Large object of size 2.12 MB detected in task graph: \n",
      "  ({'pdf': 'JVBERi0xLjUNCiW1tbW1DQoxIDAgb2JqDQo8PC9U ... 6.00199.pdf'},)\n",
      "Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "\n",
      "    future = client.submit(func, big_data)    # bad\n",
      "\n",
      "    big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "  % (format_bytes(len(b)), s)\n"
     ]
    }
   ],
   "source": [
    "client.scatter(pdfs) # PDFs have lots of bytes, good to scatter them first\n",
    "pdf_images = [client.submit(pdf_to_images, pdf) for pdf in pdfs]\n",
    "pdf_images = [p.result() for p in pdf_images]\n",
    "images = [i for j in pdf_images for i in j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propose regions on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals = client.map(process_page, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify the proposals\n",
    "\n",
    "We load the model from pretrained weights, and supply the model config. We'll run the model on CPU, but if you have a GPU available, you can load the model to your GPU by setting the device string. To enable parallelism properly, we will load a model onto each of the workers by adding a model plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axg160/miniconda3/envs/cosmos2/lib/python3.6/site-packages/ingest/process/detection/src/torch_model/model/utils/config_manager.py:22: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = yaml.load(fh)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== BUILDING MODEL ======\n",
      "Built backbone resnet50\n",
      "Building downstream components via shape testing\n",
      "done shape testing, building, attention mechanisms\n",
      "built multi head attention\n",
      "8, 8, 2048\n",
      "super called\n",
      "going to build a 131072 by 1024 matrix of weights\n",
      "built embeddings\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axg160/miniconda3/envs/cosmos2/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:58667': {'status': 'OK'},\n",
       " 'tcp://127.0.0.1:58668': {'status': 'OK'},\n",
       " 'tcp://127.0.0.1:58670': {'status': 'OK'},\n",
       " 'tcp://127.0.0.1:58671': {'status': 'OK'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = '../cosmos/ingestion/ingest/process/configs/model_config.yaml'\n",
    "weights_pth = '../cosmos/weights/model_weights.pth'\n",
    "device_str = 'cpu'\n",
    "plugin = DetectPlugin(model_config, weights_pth, device_str, keep_bytes=False)\n",
    "client.register_worker_plugin(plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_objs = client.map(detect, proposals)\n",
    "detected_objs[0].result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is the minimal detection pipeline to detect tables, figures, etc. We provide additional ways to improve the model, such as an XGBoost post processing step and a rule-based postprocessing step, but that is beyond the scope of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
